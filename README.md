#Artmosphere
Website: www.artmosphere.nyc

<img src="https://github.com/keiraqz/artmosphere/blob/master/img/cover.png" alt="alt text" width="600" height="350">


##Table of Contents
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#introduction">Introduction</a>
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#settings">Settings</a>
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#data-processing">Data Processing</a>
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#live-demo">Live Demo</a>
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#presentation-deck">Presentation Deck</a>
- <a href= "https://github.com/keiraqz/artmosphere/blob/master/README.md#instructions-to-run-this-pipeline">Instructions to Run this Pipeline</a>


##Introduction
This is a data engineering project at <a href= "http://insightdataengineering.com/">Insight Data Engineering Fellow Program</a>. The project provides a platform for users to search for different artworks, see similar art pieces and real-time popularity of a given art piece. Users can also see where all the artworks have been uploaded across the world. The main goal of the program to learn different tools used in a data pipeline for processing large datasets in a distributed manner.


**Tools used:**
- Data ingestion: <a href= "http://kafka.apache.org/">Kafka</a>
- Data storage: <a href= "https://hadoop.apache.org/">Hadoop Distributed File System</a>
- Batch processing: <a href= "https://spark.apache.org/">Spark</a>
- Real-time processing: <a href= "https://spark.apache.org/streaming/">Spark Streaming</a>
- Database: <a href= "https://www.elastic.co/products/elasticsearch">Elasticsearch</a>, <a href= "http://cassandra.apache.org/">Cassandra</a>
- Web API: <a href= "http://flask.pocoo.org/">Flask</a>
- Website: <a href= "http://getbootstrap.com/">Bootstrap</a>, <a href= "http://www.highcharts.com/">Highcharts</a>


##Settings
**Dataset:**
The dataset is a collection of 26,000 artworks and 45,000 artists collected from <a href= "https://www.artsy.net/">Artsy.net</a> in JSON format. In order to simulate real-time user activities, the project also used self-engineered data in two formats:
- Collecting log: timestamp, user\_id, collected, artwork\_id
- Uploading log: timestamp, user\_id, uploaded, artwork\_id, location\_code


**AWS Clusters:**
A distributed AWS cluster of 4 EC2 machines is being used for this project. All the components (ingestion, batch and real-time processing) are configured and run in distributed mode, with 1 master node and 3 slave nodes. The master node has 8GB of memory and 50GB of storage. The work nodes each has 8GB of memory and 1TB of storage.


##Data Processing
<img src="https://github.com/keiraqz/artmosphere/blob/master/img/pipeline.png" alt="alt text" width="600" height="300">

- **Data Ingestion (Kafka):** The datasets for batch and real-time processing are ingested using Kafka. 
  - Related files: <a href= "https://github.com/keiraqz/artmosphere/blob/master/kafka/my\_streaming\_producer.py">Streaming producer: my\_streaming\_producer.py</a>, <a href= "https://github.com/keiraqz/artmosphere/blob/master/kafka/hdfs\_consumer.py">hdfs_consumer.py</a>.

- **Batch Processing (HDFS, Spark):** A kafka consumer stores the data into HDFS. 

- **Serving Layer (Elasticsearch, Cassandra):** Datastore tables store the aggregate views for hour of day, day of week and individual cab profiles as generated by Hive. The table schema is optimized for quick access, by storing the hours as columns and the totals for each day in the same row (this way, hourly and daily profiles can be served from the same table/rows).

- **Speed Layer (Spark Streaming):** The topology for processing real-time data comprises of a kafka-spout and a bolt (with tick interval frequency of 5 sec). The data is filtered to only store currently available (unoccupied) cabs into HBase. In order to serve queries with low latency, all the data is stored in one row (maximum possible columns = number of cabs = 500). For future work, the data can be stored with the key as city, so that all cabs pertaining to a city can be retrieved via one row. If the number of cabs is large, further breakdown on city_zipcode (as key) will enable quick access, while retaining the advantage of quick row scans in HBase.

- **Front-end (Flask):** The cab locations are rendered on Google Maps and updated at 2 sec interval via AJAX. Historical data is represented as bar and line charts. Realted files: <a href= "https://github.com/PreetikaKuls/Insight-MapMyCab/blob/master/flask/app/views.py">views.py</a>, <a href= "https://github.com/PreetikaKuls/Insight-MapMyCab/blob/master/flask/app/static/batch.js">batch.js</a>, <a href="https://github.com/PreetikaKuls/Insight-MapMyCab/blob/master/flask/app/static/map.js">map.js</a>.


##Live Demo:
A Live Demo of the project is available here: www.artmosphere.nyc
- The artwork information:

<img src="https://github.com/keiraqz/artmosphere/blob/master/img/art_info.png" alt="alt text" width="600">

- Display similar artworks:

<img src="https://github.com/keiraqz/artmosphere/blob/master/img/similar.png" alt="alt text" width="200">

- Plots show in real-time how many people have collected this piece of art within a 5-sec frame

<img src="https://github.com/keiraqz/artmosphere/blob/master/img/trend.png" alt="alt text" width="300">
<img src="https://github.com/keiraqz/artmosphere/blob/master/img/trend2.png" alt="alt text" width="300">

- A map shows where all the artworks have been uploaded across the world:

<img src="https://github.com/keiraqz/artmosphere/blob/master/img/map.png" alt="alt text" width="600">

##Presentation Deck
The presentation slides are available <a href= "http://www.artmosphere.nyc/slide">here</a>.

##Instructions to Run this Pipeline

Install python packages:
```sudo pip install kafka-python happybase pyleus mrjob```

Run the Kafka producer / consumer:
```python kafka/producer.py```
```python kafka/kafka_consumer.py```

Run MrJob:
```python mr_hourly_job.py -r hadoop --hadoop-bin /usr/bin/hadoop hdfs:///<input file path> -o <output file path>```

Run Hive Scripts
```hive -f <filename>```

Build storm topology:
```pyleus build cab_topology.yaml```

Submit pyleus topology:
```pyleus submit -n 54.153.51.200 cab_topology.jar```






